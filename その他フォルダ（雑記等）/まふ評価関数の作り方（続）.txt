第５回電王戦までまだ３ヶ月もあるし、作り方はどんどん公開していく方針で。


■今現在配布しています「mafu」＝まふ評価関数として認知されつつある？評価関数ですが、これ加工練習用で作成したものです。

配布前に作成したオリジナル評価関数は手元対局で、relmo8に勝率６０％ありました。

で、次の日にその評価関数相手に勝率６０％の物が出来ましたので、現在非公開の物はrelmo8に勝率７０％相当です。
（ただ短時間対局の結果なので、実際はもうちょっと悪いかもしれません）

単純にやね氏のブログ通りの勝率計算を当てはめると、elmo(WCSC27)＋Ｒ２５０相当になるかと思われます。

さらに非公開「まふ戦型別定跡」がelmo(WCSC27)相手に＋Ｒ２００相当なので、最高レートだと定跡使用でその分も加算されます。
（評価関数がここまで育つとほとんどレート加算されないかも。未確認）

私の現状がこんなところです。


■とりあえず評価関数作成で何をやったのか

私の考えでは、
depth６とか２手先で評価値４００とか動くときが結構あるので、普通に終局までの評価って不可能であり、結局自己対局型の教師生成だと
統計的に寄っていく先手勝率５４％、後手勝率４６％程度の差（８％）でしか機械が上手く学習出来ていないのではないか？と仮説を立ててみました。
（例えばelmoが学習した５０億局面だと、実際上手く学習出来た局面は４億局面で、残りの４６億局面は正と負の値が拮抗したほぼ学習出来ない局面）

なので、実際に将棋で指された（人間＋ソフト）の指し手を好手、悪手関係なく５００万手を再評価してみました。
定跡登録で採用率をすべて均等にし、２０手、３０手、４０手、５０手・・・と作って、投了値を１００として定跡を抜けた瞬間の局面、
細かい評価値はともかくどっちが良いかくらい分かるだろうと再評価しました。
elmo式だとこれで勝敗のボーナスが正しく付くんじゃないかなー？とやってみました。

これだと教師局面の生成も超高速で出来ました。
それをdepth６×３億局面の学習１周で実施。

効果はバツグンで勝敗のボーナスがとても大きく付きました。（評価関数の特徴がモロに出た感じ）
ただしこのままだと一部の局面評価値にＰＶがハマルので、勝率としてはそんなに変わりませんでした。

このデコボコを調整すると、relmo8に勝率＋１０％となりました。


■そもそも５００万手の評価でそんなに勝率があがるの？

これは以前、読み太塚本さんとメールでディスカッションした話ですが、

＞局面合流については過去の定跡を配布したReadmeとかに書いていたのでそのまま書いていましたが詳しく説明すると、
「まふ定跡」作成時にアマ３段（Ｒ２１００）〜アマ５段（Ｒ２５００）から１０万局の棋譜を提供して頂きました。（振り飛車党３０％程度）
それを６０手目まで登録すると、１０万×６０手＝６００万手登録したはずが、２４０万手の登録となり、この時点でおよそ６０％程度が局面合流していました。
そしてコンピューター将棋はほぼ居飛車党と言って間違いないので、居飛車党に限定すると１４０万手程度になりました。
そこから「まふ定跡」作成を始めてver7までに、性質の違う技巧と浮かむ瀬というソフトを技巧１７００局＋浮かむ瀬１５００局程度の棋譜を調整したところ、
１２０万手になりました。（人間の棋譜から減った２０万手は削除しても問題ないと思うような指し手だった）
この結果から、将棋の居飛車党の採る戦法は全体の５分の１程度（１２０万手÷６００万手）となりました。
そして調度「浮かむ瀬」のdepth６＝Ｒ２１００程度で、depth８＝Ｒ２５００程度であることから、現在やっている教師局面に近い値ではないかと思っているので、何かの参考になれば良いかなと書きました。

と、なっており意外に必要なパラメタは少ないのです。

やねうら王開発者のように、完全教師無しで学習していく場合は何十億局面というビックデータが必要だと思いますが、
そういった特殊な条件でもなければ何百年の歴史があり、たくさんの実践譜が存在する将棋を学習するのに必要な教師局面は意外と少ないと思われます。

棋譜のデータベース化が進んでいなかったBonanza時代ならともかく、現在の状況下でビックデータが必要だと言っている開発者は……。
